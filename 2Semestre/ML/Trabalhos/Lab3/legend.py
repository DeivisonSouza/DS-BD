"""
=======================
Whats New 0.98.4 Legend
=======================

Create a legend and tweak it with a shadow and a box.
"""
import matplotlib.pyplot as plt
import numpy as np


ax = plt.subplot(111)
x=[1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 13000, 14000, 15000, 16000, 17000, 18000, 19000, 20000]

knn_mod=[0.79176755447941893, 0.86684513862838042, 0.89010333185554003, 0.89383760188248129, 0.89774238652252503, 0.92045493298775705, 0.92691743682433581, 0.9308733758483102, 0.93455649149132081, 0.93822255567302115, 0.93808614398253931, 0.9386658936670873, 0.93868294512839745, 0.93817140128909049, 0.93742113699144014, 0.93812024690515983, 0.93736998260750948, 0.93687549022951266, 0.93776216621764485, 0.93999590764928553]

linear_mod =[0.72458479691709576, 0.77446032124953113, 0.79337039184258096, 0.80204958564949014, 0.80740374450090369, 0.82769498346008252, 0.84786686219008967, 0.86960747536063843, 0.87965078607236635, 0.88674419397742388, 0.88674419397742388, 0.887119326126249, 0.88994986870374793, 0.88938717048051019, 0.8894212734031306, 0.89370119019199945, 0.89772533506121477, 0.9010503700167104, 0.90384680967158881, 0.90691607270743102]

Gaus_mod =[0.67435119189714554, 0.78201411860996484, 0.83842035262421988, 0.84489990792210889, 0.85509668178562903, 0.86935170344098489, 0.87424547283702214, 0.88248132864986528, 0.88735804658459227, 0.88908024417692599, 0.88740920096852305, 0.88638611328990891, 0.88870511202810076, 0.88546533437915631, 0.88442519523923202, 0.8858916209119122, 0.88643726767383968, 0.88805715649831185, 0.88851754595368826, 0.88908024417692599]

LDA_mod =[0.78731712307744772, 0.87327353954233877, 0.89354772704020735, 0.88536302561129487, 0.89052961838829592, 0.90251679568939058, 0.91244074617194693, 0.92076185929134124, 0.92371176209801176, 0.92785526719639877, 0.92606486375882413, 0.92454728370221329, 0.92422330593731883, 0.92204071888960881, 0.92083006513658217, 0.9210517341336153, 0.92314906387477402, 0.92505882754152036, 0.9264229444463391, 0.92785526719639877]

perceptron_mod_it25 = [0.7828155372915458, 0.8867100910548034, 0.9161238618149575, 0.8894042219418204, 0.918852095624595, 0.918272345940047, 0.9304470893155543, 0.9249053643897281, 0.9105650854278212, 0.939740135729632, 0.9447191624322204, 0.9414452818606555, 0.9415475906285169, 0.8992940695017563, 0.9375575486819221, 0.9255362684582069, 0.9445145448964977, 0.9425536268458207, 0.9359035569348293, 0.9497663949800498]

perceptron_mod_it50 = [0.80494833407223, 0.8781502574770658, 0.9181870886334959, 0.894400300105719, 0.910189953278996, 0.9320328752174062, 0.9020223033113938, 0.9110084234218873, 0.931947617910855, 0.9480100944650957, 0.9439006922893292, 0.9317088974525117, 0.9462026395662109, 0.9339767418067728, 0.9374040855301299, 0.9381714012890905, 0.9334481465061556, 0.9509599972717662, 0.945554684036422, 0.9461685366435904]


perceptron_mod_it75 = [0.811649558367152, 0.8868123998226648, 0.9245643351635235, 0.8992940695017563, 0.9193636394639021, 0.9344541827234594, 0.9327319851311258, 0.9425706783071309, 0.928605531494049, 0.9391603860450841, 0.936466255158067, 0.9428264502267845, 0.9434061999113325, 0.9415646420898272, 0.9495958803669474, 0.9459809705691777, 0.9453330150393888, 0.9523241141765849, 0.9409337380213484, 0.9326637792858848]


perceptron_mod_it100 = [0.811649558367152, 0.8940422194182042, 0.9255362684582069, 0.9180847798656345, 0.9217849469699553, 0.9346246973365617, 0.9227739317259489, 0.934061999113324, 0.9393820550421171, 0.9429628619172663, 0.9453671179620093, 0.9451283975036661, 0.9431674794529891, 0.9480953517716468, 0.9432015823756096, 0.9460832793370392, 0.9534665620843706, 0.9503120417419773, 0.94978344644136, 0.95196603348907]

perceptron_mod_it125 = [0.811649558367152, 0.9001977969511987, 0.9234048357944276, 0.9210858370562357, 0.9062169627937114, 0.9353579101729018, 0.937335879684889, 0.9351021382532483, 0.9422978549261671, 0.9474814991644784, 0.9510793575009379, 0.9411724584796917, 0.9419738771612727, 0.9337550728097398, 0.9521535995634826, 0.9313849196876173, 0.9520001364116905, 0.9333458377382942, 0.9545578556082256, 0.9287760461071514]

perceptron_mod_it150 = [0.811649558367152, 0.9011015244006412, 0.924598438086144, 0.91339562800532, 0.894400300105719, 0.9375063942979913, 0.9376428059884732, 0.9439347952119497, 0.9459639191078676, 0.936244586161034, 0.9313167138423762, 0.9482658663847492, 0.95123282065273, 0.9475326535484091, 0.9471745728608941, 0.9193124850799713, 0.9534495106230604, 0.9331412202025714, 0.9511475633461788, 0.9501074242062545]

#plt.plot(x , perceptron_mod_it25, label="max_iter = 25")
#plt.plot(x , perceptron_mod_it50, label="max_iter = 50")
#plt.plot(x , perceptron_mod_it75, label="max_iter = 75")
#plt.plot(x , perceptron_mod_it100, label="max_iter = 100")
#plt.plot(x , perceptron_mod_it125, label="max_iter = 125")
#plt.plot(x , perceptron_mod_it150, label="max_iter = 150")
plt.plot(x , perceptron_mod_it50, label="Perceptron (max_iter = 50)")

plt.plot(x , knn_mod , label="KNN")
plt.plot(x , linear_mod , label ="Logistic Regression")
plt.plot(x , Gaus_mod , label="Gaussian Naïve Bayes")
plt.plot(x , LDA_mod , label="LDA")
plt.axhline(y=0.9480100944650957, color='r', linestyle='--')
plt.axvline(x=10000, color='r', linestyle='--')

#plt.title('Perceptron')
plt.title('Comparação de classificadores')
plt.xlabel('Batchsize')
plt.ylabel('Accuracy')
plt.legend()


#ax1 = plt.subplot(212)
#x=[1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 13000, 14000, 15000, 16000, 17000, 18000, 19000, 20000]

#knn_time=[39.004700899124146, 68.23262000083923, 94.06645512580872, 129.77866101264954, 162.08356285095215, 190.0624749660492, 211.45835614204407, 237.59369087219238, 254.95228505134583, 279.9081621170044, 302.4850962162018, 321.43626713752747, 336.47098898887634, 350.8941788673401, 369.7603089809418, 365.99907207489014, 374.31050300598145, 383.07978892326355, 395.0217270851135, 414.4825818538666]
##Gaus_time =[2.3461790084838867, 2.3523569107055664, 2.347378969192505, 2.3418498039245605, 2.357410192489624, 2.349074125289917, 2.35503888130188, 2.3513381481170654, 2.370041847229004, 2.368175983428955, 2.3639028072357178, 2.3717620372772217, 2.3721089363098145, 2.3710310459136963, 2.3857760429382324, 2.375216007232666, 2.3823840618133545, 2.3791391849517822, 2.385517120361328, 2.3899710178375244]
#LinearDis_time =[0.45839595794677734, 0.544374942779541, 0.6058220863342285, 0.6936450004577637, 0.7730200290679932, 0.8409309387207031, 0.9359140396118164, 1.0146801471710205, 1.0953421592712402, 1.1650240421295166, 1.2618038654327393, 1.3284499645233154, 1.4148790836334229, 1.499169111251831, 1.5953660011291504, 1.691603183746338, 1.7784228324890137, 1.8409199714660645, 1.925339937210083, 2.014470100402832]
#perceptron_time = [0.4595499038696289, 0.576246976852417, 0.7741367816925049, 0.9720160961151123, 1.153968095779419, 1.3345508575439453, 1.526643991470337, 1.7005250453948975, 1.8971271514892578, 2.0725438594818115, 2.2614529132843018, 2.4235270023345947, 2.594542980194092, 2.7907071113586426, 2.959355115890503, 3.153109073638916, 3.328022003173828, 3.541383981704712, 3.7185540199279785, 3.8863911628723145]

#plt.plot(x , knn_time , label="KNN")
#plt.plot(x , linear_time , label ="LM")
#plt.plot(x , Gaus_time , label="GAUS")
#plt.plot(x , LinearDis_time , label="LDM")
#plt.plot(x , perceptron_time , label="LDM")

#plt.xlabel('size')
#plt.ylabel('Time (s)')
#plt.legend()


plt.show()

#############################################################################
#
# ------------
#
# References
# """"""""""
#
# The use of the following functions, methods, classes and modules is shown
# in this example:

import matplotlib
matplotlib.axes.Axes.legend
matplotlib.pyplot.legend
matplotlib.legend.Legend
matplotlib.legend.Legend.get_frame
